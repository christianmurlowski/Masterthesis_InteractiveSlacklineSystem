\section{System architecture}\label{5_3_systemArchitecture}
%- System architecture of system --> Unity3D, Kinect SDK, Kinectstudio, VGB --> kinect sdk free to use since version X
In the following several components of the general system architecture will be described that are necessary for the functionality of the interactive learning system with real-time feedback and for the study afterwards. An overview can be seen in figure \todo{systemarchitecture}.

\subsection{Hardware and software components}
% Kinect, Beamer, Screen, Slackline --> Alpidex High Performance

As slackline the mobile \textit{alpidex POWER-WAVE 2.0} is used. It is placed in front of the \textit{Microsoft Kinect v2}, which is used as tracking device. The Kinect itself is attached on a \todo{modell} tripod with a height of about \todo{height, hüfthöhe?}. A \textit{Steambox PC} \todo{footnote specs} served as development device that fulfilled the recommended specs of the Kinect: \todo{kinect specs}. To give the user a more immersive feeling a projector \todo{modell} with a resolution of 1920x1080 was attached on a traverse system. With this a projection screen with a size of \todo{2x3m} is used.

% KinectStudio, VGB, Unity3D, Kinect SDK for unity, Kinect MS-SDK
For software realization the cross-platform game engine \textit{Unity3D} by \textit{Unity Technologies} is used. This is a game engine which is mostly used for developing
Applications developed with this can be deployed for several platforms on desktop, mobile, web, console, TV, VR, and AR (e.g. Windows, macOS, Android, iOS, Oculus Rift, Windows Mixed Reality and so on). It is widely used for developing games but has also very high compatibility with devices like \textit{VR Head Mounted Display}, \textit{Leap Motion}, or the Kinect.

 For accessing the data stream of the Kinect the \textit{Microsoft Kinect SDK v2.0}
\footnote{\label{fn:kinectTools}\url{https://developer.microsoft.com/de-de/windows/kinect/tools}}has to be installed on the PC. Microsoft offers also a \textit{Kinect for Windows Unity package}\cref{fn:kinectTools}, which delivers all required scripts to manage the data stream in Unity for creating a Kinect based unity application. Since \textit{Unity 5} it can be used with the free personal edition, whereas before it could be only used with the pro version. Also the \textit{Kinect v2 Examples with MS-SDK} \footnote{\url{https://www.assetstore.unity3d.com/en/\#!/content/18708}} by Rumen Filkov were used for making data access and interaction implementation more simple as well as getting an idea on how to handle incoming data from the Kinect.

\subsection{Software implementation}
The software development process consists of the interplay of two system components (figure \todo{image unity and kinect sdk}). First Unity itself, which is used to display and manage actions by the user on the interface. Second the Kinect SDK plugin for accessing the users action recognized by the Kinect device. In the following the data structure and management, interaction integration, and feedback implemention of the system are further described.

%This application a 3D project has been generated, since the Kinect uses 3D space to track the user, she should be able to interact within that, and a 3-Dimensional environment design is considered. Further some examples of the interaction implementation is described.
\subsubsection{Data management}
The data will be stored in JSON files, which is more human-readable and makes accessing and updating data simple in Unity via the JSON serialization feature~\footnote{\url{https://docs.unity3d.com/Manual/JSONSerialization.html}}. For each object containing key-value pairs a class is generated, i.e. for Users, Tier, Exercise, Sides, and Repetitions. In listing \todo{ref c\# code example} an example for the \textit{Tier} object can be seen. A class must be marked as serializable to work with the JSON serializer. It contains variables, which match the JSON structure on listing \todo{ref json code}. 
\todo{c\# code example and json file}

With this an instance of the class can be created and the value accessed for adjustments (listing \todo{ref img class instance}). This can then be serialized into a JSON object. Line \todo{ln number} shows how existing JSON data can be converted back into an object instance. This is needed to fetch user data that already exists.

\todo{img class instance}

In the system a default exercise JSON file was created that is used as reference for all users. Each user has therefore the same basement regarding the exercises right from the beginning. For proper data management a internal exercise and user editor was created in Unity (figure \todo{ref editor window}). The data files can be managed and adjusted more easily for testing purposes.

\todo{img editor window}

The files are stored within the \textit{StreamingAssets} folder of unity. It serves as location where files that should be accessed via path name of the target machines file system should be stored. The overall data structure can be seen in figure \todo{img ref data structure}.

\todo{image of data structure}

\subsubsection{Engagement gesture}
The engagement gesture is the very first interaction with the system. The user raises her hand, such that she initially knows that the system recognizes her actions. An example on how to acess the Kinect and manage this gesture can be seen in the code snipped in listing~\ref{lst:codeEngagement}. The \textit{KinectManager} exists as an empty \textit{GameObject} in the scene and has to be referenced in the script. The \textit{KienctInterop} class delivers several utility and interop function and calls the sensor interfaces. Here it is used to assign the proper joint type for tracking them. For the functionality the script has to assure that the Kinect is initialized, a user has been detected, and the relating joints are recognized. In line\textit{~\ref{lst:codeEngagement15}} the condition checks if the current vertical position of the right hand is greater than the current position of the head joint. If this is the case, the next scene is loaded. This is actually part of the script for the first scene in the application, in which the user has to engage with the Kinect by doing the described movement.

\begin{lstlisting}[caption=C$^\sharp$ example code for tracking a raising hand, label=lst:codeEngagement]
public  KinectManager KinectManager;
private KinectManager _kinectManager;
private KinectInterop.JointType _jointHandRight, _jointHead;

void Start ()
{
	_jointHandRight = KinectInterop.JointType.HandRight;
	_jointHead = KinectInterop.JointType.Head;
}
void Update ()
{
	if (KinectManager == null) return;
	_kinectManager = KinectManager.GetComponent<KinectManager>();

	if (_kinectManager && 
		  _kinectManager.IsInitialized() && 
		  _kinectManager.IsUserDetected())
	{
		long uId = _kinectManager.GetPrimaryUserID();

		if ((_kinectManager.IsJointTracked(uId, (int) _jointHandRight) && 
			   _kinectManager.IsJointTracked(uId, (int) _jointHead))
		{
			Vector3 jointPosHandRight = 
			 _kinectManager.GetJointKinectPosition(uId, (int) _jointHandRight);
			Vector3 jointPosHead = 
			 _kinectManager.GetJointKinectPosition(uId, (int) _jointHead);			

			if (jointPosHandRight.y > jointPosHead.y)(*@ \label{lst:codeEngagement15} @*)
				SceneManager.LoadScene("Tutorial");
		}
	}
}
\end{lstlisting}
\subsubsection{Starting position}\label{5_2_1_startingPosition}
After engaging with the system the user  is introduced in how to stay in the proper starting position. This is required by some actions like just before starting the exercise execution. It ensures the user is ready and to get her initial joint positions for primarily calculating the y-axis of the joint (cf. \todo{section Feedback integration}). This initial joint position can differ if the user stands closer or more far away from the sensor because of the Kinects angel regarding its height. Hence the z-axis joint position of the left and right feet will be compared with each other to have the same value with a little tolerance. Hereby the user has to stand with both feets in line and in front to the Kinect like in figure \todo{ref illustration}.

\todo{illustration feets parallel}

\subsubsection{Hand cursor interaction}
To be more familiar with the system the user interacts with her own hands. With this she is in constant system interaction and gets more used to it. A cursor visualizes where the user currently is and with this she can interact with other interface elements. As interaction gesture four different approaches were tested. First hovering with the hand over elements (Figure \todo{interaction variation A}). This gesture is a good approach specific small elements should be selectable. The problem was that relatively big and many interaction elements exists. Therefore the user accidently hovers over elements which resulted in unwanted misclicks. 
The second one is interacting by closing the hand to a fist (Figure \todo{interaction variation B}). In testing periods many user accidently triggered a click although they thought they did nothing wrong. Problems exist here concerning the users anatomical behaviour. The difficulty was that the users hand, closes automatically during hand movement. No user will stretch and spread her hand out because it is uncomfortable. Hence they tend to close the hand a little bit, which is more comfortable and results in a misclick recognition of the Kinect, which is fortified by the enlarged standing position of the user (Figure \todo{comparison default \& target }).
A better interaction gesture is the \textit{V-sign}. The user makes simply a pointing gesture with the index as well as the middle finger (Figure \todo{interaction variation C}). The event is triggered when the user releases into the default hand state. It clearly distinguished from the fist gesture and will be accurately recognized. To give the user appropriate feedback the state is visualized like in (Figure \todo{img handcursorPointing}).
The last gesture is pushing the hand towards the Kinect. This is related to the real world like pushing a button down and therefore the most intuitive gestures (Figure \todo{interaction variation D}). 
%The state of the this interaction gesture is visualized by providing a filling circle around the hand cursor that represents the progress like seen in figure \ref{fig:handcursorProgress}.
Since in this gesture the hand has to move from point a to point b in the z-axis the progress is visualized by a filling circle like seen in figure~\ref{fig:handcursorProgress}.

The pushing gesture is implemented as the main interaction technique because of its natural movement and real world 
reference. A fallback gestures is also implemented if something went wrong or the user feels uncomfortable with the pushing gesture. The \textit{V-sign} is the next best technique for this, since there were less problems than with the remaining gestures.

\todo{img interaction varations A - Hover\&wait, B - fist, C - pointing, D - push}

\begin{figure}[htb]
	\centering
	\begin{minipage}[t]{1\linewidth}
		\centering
		\includegraphics[width=0.6\linewidth]{Pictures/handcursorProgress}
		\caption{Progress of handcursor (Left: Default, Middle: In progress, Right: Finished)}
		\label{fig:handcursorProgress}
	\end{minipage}
\end{figure}

\subsubsection{User selection}
The user profile are created by the developer. In the user selection the trainee should select the right profile by herself to cover the condition of multiple user that can train with the system. When selecting a user the system checks if the JSON file has any data. If no data is available the default exercise JSON file will be copied. The users name and id are saved in as \textit{PlayerPrefs}. This is a class in Unity that provides the possibility to be globally accessible in the application. The respective code for setting and getting the values can be seen in listing \todo{ref listing playerpfrefs}. This is needed to find the correct path for the JSON files to save the data.

\todo{listing playerprefs}

\subsubsection{Tier \& exercise menu}
The tier and exercise menus are designed as levels the user has to accomplish. A locked tier can be unlocked by executing all exercises of the previous one. This is done by storing a boolean variable named \textit{accomplished} for every tier and its exercises. The structure already seen in section \textit{\ref{4_4_exercises}} is adapter for storing the corresponding information for the tier, exercise, side, or repetition.

\subsubsection{Exercise execution, providing feedback}
The most functionality is implemented in the exercise execution. Feedback indicators provide the user with information about the current exercise in real time. This should support the user to successfully accomplish the exercise performance. In the slackline system the following feedback indicators are integrated:

\begin{itemize}
\item Correct performance of an exercise
\item How good the exercise is currently performed, namely the confidence
\item Elapsed time the user is performing the exercise
\item When the repetition was successful (i.e. minimum time has been reached)
\item When a repetition attempt was not successful
\item Amount of repetitions in general, finished, and left
\item Checklist about key elements of an exercise (hands up, foot stretched, etc.)
\end{itemize}

After each successful exercise execution the user is lead to a summary screen. Here she gets an overview about her performance. It shows parameters about the execution time, attempts, and the confidence for each repetition as well as an average value for the accomplished side. A similar summary screen exists also for the entire stage with the same parameters but only the average value for the entire exercises.

\begin{comment}
\item When an exercise is currently correctly performed
\item How good the exercise is currently performed, namely the confidence
\item The elapsed time the user is performing the exercise
\item When the repetition is successfully accomplished, i.e. the minimum time has been reached
\item When an repetition attempt was not successful
\item How many repetitions in general, finished and left
\item Checklist about key elements in an execution (like hands up, foot stretched, etc.)
\item A summary that shows the user parameters about the performance (execution time, overall attempts, confidence) for each repetition and an average value of these
\item A similar summary can also be found for the entire stage, where the same parameters for each exercise are listed in average
\end{comment}