\section{Gesture Construction}\label{5_2_gestureConstruction}
%- Recording of gestures --> Kinectstudio --> Making/Train gestures --> Visual gestures builder
The system should provide predefined exercise on which the user is guided for learning slacklining. To give feedback in an appropriate manner the exercises are represented as custom gestures. There are two approaches of creating custom gestures. The first is one is to do heuristics, which means manually tracking the position of each joint and write code according to the action that should happen if the joints exceed a threshold or are in a defined range. This works either if the gesture is very simple, e.g. raising the user hand over her head or if the developer has a good understanding about how the human body behaves in more complex gestures. 

In the most cases developer have not the appropriate expertise for coding complex gestures. Therefore the second approach is to use the Visual gesture builder (VGB) provided by Microsoft. This tool relies on machine learning and looks at the data given by the developer via pre recorded clips. With these it builds a database that can then be used to track the actual gesture in an application. The more data is provided to it the better the detection by the Kinect. Another advantage is that environmental factors are not that complex to handle as in comparison to heuristics. For example if the sensor is too high or too low the developer has to consider this in his code and it can blow up very fast managing and maintaining such factors. With VGB the developer just records data with the sensor on a higher or lower level and feed it to the machine learning algorithm. The cons are the huge file size of the recorded clips which can take very much disk space. Also setting the keyframes for parts that the builder should detect is time consuming whereas on the other hand it is simple and user friendly.

The workflow looks like the following. First clips have to be recorded with Kinectstudio. This is a monitor and recording software for the kinect. The clip has to be recorded in raw data to have the necessary information about the streams. Like seen in figure \todo{figurename} several streams can be selected as well as recording audio and so on. The most important for the VGB are the \todo{infrared, body, bodyindex?}. After finishing with this a new project can be built with the VGB. This can be done either manually in one screen or with a wizard that guides the user through the process. In here the developer considers on which parts of the body the gestures relies on. After that she has to choose if it is a discrete or a progress gestures \todo{explain both}. Discrete gestures area more of static ones, like staying on one leg or punching. Progress gestures can have multiple steps like switching the standing leg.
Figure \todo{VGB project hierarchy} shows the project hierarchy. In part A of figure \todo{VGB project hierarchy} the actual training data, the recorded clip of Kinect studio, is inserted. With this the developer has to set keyframes that define the gesture movement to track. If it is a progress gesture more fine granular keyframes, e.g. for turning a steering wheel from the right side to the left are possible. After finishing this the database can be built and then analyzed via the live preview that can be seen in figure \todo{figure live prev} or with other recorded clips in the analyze area. With this the VGB sets its own keyframes on the clip regarding the database it is tested. The developer can then accept these, adjust it, and move it into training clips if necessary.

The database can then be implemented in the application for gesture detecting. The structuring of the systems architecture is part of the next section.
